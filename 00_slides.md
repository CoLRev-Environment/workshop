---
marp: true
header: 'CoLRev workshop at ECIS 2024'
footer: 'Gerit Wagner and Julian Prester'
theme: ub-theme
paginate: true
---

# Welcome to CoLRev

---

# Before we start

- Go to the [example repository](https://github.com/CoLRev-Environment/colrev-template)
- Start it in Codespaces

![bg right:50% width:550px](../assets/start-codespaces.png)

---

# Who are we?

- Gerit Wagner: short bio

- Julian Prester: short bio

Overview of publications on literature reviews, tools, teaching (phd, bachelor, master), editorial work, ...

---

# Literature reviews with CoLRev

First slides: what do we mean with colrev/what's our focus?
colrev: literature reviews in collaborative settings

- All steps / something we discussed earlier, when announcing the workshop (record keeping, put users in a position to report a full standalone paper at all times)
- Different types of reviews (not just SLR)
-> Extensible approach, adapting the first steps with parameters, and selecting different packages for the data analysis/extraction/coding/synthesis/RoB
- Reusable: it should be easy to update prior reviews, import samples from review papers published by another author team, or student papers etc. (reuse: one step further than reproducibility)

---

# Collaborative literature reviews

Reliability
- open data, open-source code (transparency - using Git to see exactly what was changed) - not the most common approach in the context of LR
- validation of data and code (manual and algorithmic)
- easy undo operations
-> paradigm change: no longer require "blind trust" in algorithms/student assistants

enables / requires

Efficiency
- combining SOTA tools, testing new algorithms
- involve colleagues, student assistants, crowds (variance in quality, availability, cost)

---

# Tutorial

- small groups
- codespaces: read doc + enter commands
- GW: prepare worksheet (printed + in codepaces)

---

# Problem formulation

colrev init (with review types, and protocol for notes)

---

# Search

- search: interactively add search (DB + API)
- IEEE (web of science) - publicly accessible download results files
- AIS via api (TEST)

Topic: "microsourcing" (or a topic of your choice, preferrably one that does not return too many results)

---

# Screen


Includes dedupe

dedupe: highlight: single open-source (code  peer reviewed) tool

PDF retrieval

Screen (full-texts)

---

# Data extraction, analysis, synthesis

default workflow, with pdf and prisma at the end

---

# Optional items (depending on time left)

- Show search updates?
- backward-search - one or several papers? TEST
- curation (online?)? - curated metadata, pdfs: 80% of papers?
- exports to different bib formats / just the sample (TEST!)

<!-- generate profiles?! / structured data -->

---

# Thank you

- How to get in touch, ask for help (literature-review consultation hour/calendly?)
- How to get involved (report bugs, contribute, ...)